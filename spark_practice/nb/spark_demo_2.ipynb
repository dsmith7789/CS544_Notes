{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78863956-5b16-4d12-9e3c-b29af907367f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/02 02:55:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# SparkSession only takes in 1 argument, it uses these functions to set attributes and then returns itself\n",
    "# slow because it has to setup the JVM (and other steps)\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .master(\"spark://boss:7077\")\n",
    "         .config(\"spark.executor.memory\", \"512M\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5afca105-6c8d-4639-ac78-0b6db7bef4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1090023c-ad3a-4294-8487-94335d3be824",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = list(range(1_000_000))\n",
    "rdd = sc.parallelize(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99821b28-a288-49ce-8ade-513ee13e2723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 transformations, avoid the div by 0 by filtering out 0\n",
    "inverses = rdd.filter(lambda x: x > 0).map(lambda x: 1/x)    # TRANSFORMATION: perform the function on every row in the source (rdd)\n",
    "inverses    # result is still an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c68a52a-b865-42bc-98b3-8b980d6b9c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/02 02:55:50 WARN TaskSetManager: Stage 0 contains a task of very large size (2332 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.5,\n",
       " 0.3333333333333333,\n",
       " 0.25,\n",
       " 0.2,\n",
       " 0.16666666666666666,\n",
       " 0.14285714285714285,\n",
       " 0.125,\n",
       " 0.1111111111111111,\n",
       " 0.1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverses.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78520b96-4d37-45d7-a2e2-a25e86b487b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/02 02:56:50 WARN TaskSetManager: Stage 1 contains a task of very large size (2332 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4392740115605892e-05"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f01a25d0-c0b9-4a35-9dac-c41aac96e1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we're getting the 'large task' warning because we have very big partitions (1 task works on 1 partition)\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e27bb66-d3e6-49c8-afc6-6fb7cc213bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark automatically decided to use 2 partitions but we can change this\n",
    "rdd = sc.parallelize(nums, 10)\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "717e7991-b823-4f17-bdfa-ed527f633900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4392740115605814e-05"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with smaller partitions, now we don't get the warning\n",
    "inverses = rdd.filter(lambda x: x > 0).map(lambda x: 1/x)\n",
    "inverses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f11875-5129-4930-b6c5-846a05d7980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When Spark displays progress as: 4 + 2 / 10\n",
    "# 4 tasks completed\n",
    "# 2 tasks running (this should correspond to the number of cores)\n",
    "# 10 tasks total (this should correspond to the number of partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d9d44e-a301-4b3f-beed-298fdd5822f4",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fbdfb59-9e43-4154-987e-0d108c48c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample is a TRANSFORMATION\n",
    "sample = rdd.sample(withReplacement=True, fraction=0.1, seed=544)   # seed not quite deterministic, depends on partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba7ba961-70ab-4064-b8d6-5ba5bc8c20dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1776f7cc-69dd-4176-af2c-8e6a126e434e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498504.761576394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.7317185401916504"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "print(sample.mean())\n",
    "t1 = time.time()\n",
    "t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a02ed52-9a74-4887-a808-f5ead2dc1f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[7] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a3b3964-158f-4fc1-a653-0b8dde1940af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:====================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498504.761576394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.3785624504089355"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first time, the caching actually makes things slower (extra work to cache)\n",
    "# so if you're only doing it once, caching just introduces extra overhead, not worth it\n",
    "t0 = time.time()\n",
    "print(sample.mean())\n",
    "t1 = time.time()\n",
    "t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1671842c-d675-4edc-aa42-8820b21d4f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:====================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498504.761576394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.223809242248535"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after caching though, now the mean calculation using the cache is faster (don't need to resample)\n",
    "t0 = time.time()\n",
    "print(sample.mean())\n",
    "t1 = time.time()\n",
    "t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f357cb3-8fea-49f3-9bfb-9090c1c0f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not much faster though...that's because our sample is still across 10 partitions (Spark uses a narrow transformation)\n",
    "# to get truly better performance, we can tell Spark to put the sample into just 1 partition (therefore 1 task)\n",
    "sample = rdd.sample(withReplacement=True, fraction=0.1, seed=544).repartition(1).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3442cca9-88a8-4afd-b6d9-631949cfa663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498504.76157639234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.6407365798950195"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# still slow initially with the repartition\n",
    "t0 = time.time()\n",
    "print(sample.mean())\n",
    "t1 = time.time()\n",
    "t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fc1050a-b604-4475-a80e-2e22c29c84e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498504.76157639234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.106959342956543"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but now, after repartitioning and caching, the mean calculation is much faster! (6 seconds -> 1 second)\n",
    "t0 = time.time()\n",
    "print(sample.mean())\n",
    "t1 = time.time()\n",
    "t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd99a58-8c86-4426-a483-bd64ba71d005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
