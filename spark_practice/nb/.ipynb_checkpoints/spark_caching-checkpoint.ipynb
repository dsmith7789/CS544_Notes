{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11039f73-8c47-457f-9883-75cca0e23f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/04 03:21:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .master(\"spark://boss:7077\")\n",
    "         .config(\"spark.executor.memory\", \"512M\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \"hdfs://nn:9000/user/hive/warehouse\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fd578f9-83aa-40b8-8e37-c3325f95d893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'app-20231104032144-0007',\n",
       "  'name': 'cs544',\n",
       "  'attempts': [{'startTime': '2023-11-04T03:21:41.847GMT',\n",
       "    'endTime': '1969-12-31T23:59:59.999GMT',\n",
       "    'lastUpdated': '2023-11-04T03:21:41.847GMT',\n",
       "    'duration': 150653,\n",
       "    'sparkUser': 'root',\n",
       "    'completed': False,\n",
       "    'appSparkVersion': '3.5.0',\n",
       "    'startTimeEpoch': 1699068101847,\n",
       "    'endTimeEpoch': -1,\n",
       "    'lastUpdatedEpoch': 1699068101847}]}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# APIs to tell use what's going on in the Spark cluster (4040 is the port that the cluster view is on):\n",
    "\n",
    "# https://spark.apache.org/docs/latest/monitoring.html#rest-api\n",
    "# http://localhost:4040/api/v1/applications\n",
    "# http://localhost:4040/api/v1/applications/{app_id}/executors\n",
    "# look for \"totalTasks\"\n",
    "\n",
    "#r = requests.get(????)\n",
    "r = requests.get(\"http://localhost:4040/api/v1/applications\")\n",
    "r.raise_for_status()\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9a2ef36-567f-492f-b284-f6a04428597d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'app-20231104032144-0007'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_id = r.json()[0][\"id\"]\n",
    "app_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a172edf6-ec8d-4d89-995e-e95b6da3a3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executors = requests.get(f\"http://localhost:4040/api/v1/applications/{app_id}/executors\")\n",
    "executors.raise_for_status()\n",
    "\n",
    "# how many tasks has each Spark worker done?\n",
    "[exec[\"totalTasks\"] for exec in executors.json()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19332fb0-2d68-4921-a42d-7ca0c120f514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/04 03:31:56 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "# recreate the calls view\n",
    "(spark.read\n",
    " .format(\"parquet\")\n",
    " .load(\"hdfs://nn:9000/sf.parquet\")\n",
    " .createOrReplaceTempView(\"calls\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32a258a9-750b-46bb-b0b3-1847f649e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can make the tasks imbalanced between the workers by repeatedly using cached data on one worker\n",
    "df = spark.table(\"calls\").sample(withReplacement=True, fraction=0.01).repartition(1).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9748b80c-bc1e-4319-8235-bd5227059de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59912"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "126467f9-e25c-4ad6-b5df-d72d5d518984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5, 4]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executors = requests.get(f\"http://localhost:4040/api/v1/applications/{app_id}/executors\")\n",
    "executors.raise_for_status()\n",
    "\n",
    "# how many tasks has each Spark worker done?\n",
    "[exec[\"totalTasks\"] for exec in executors.json()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7fcf3fd-6956-4bb3-bba8-1a3dcdd17cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sample is cached on only one worker (we didn't use 2x replication)\n",
    "# so this will force 30 additional tasks onto 1 worker (sample only takes up 1 partition; 1 task)\n",
    "# to avoid this, use .cache(MEMORY_2) so multiple workers get assigned some work\n",
    "for i in range(30):\n",
    "    df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf5a584a-8da6-4c73-b0cb-a644fc697b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5, 34]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executors = requests.get(f\"http://localhost:4040/api/v1/applications/{app_id}/executors\")\n",
    "executors.raise_for_status()\n",
    "\n",
    "# how many tasks has each Spark worker done?\n",
    "[exec[\"totalTasks\"] for exec in executors.json()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90cb19d9-81c9-42e2-944e-14993e6f36d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (5)\n",
      "+- HashAggregate (4)\n",
      "   +- Exchange (3)\n",
      "      +- HashAggregate (2)\n",
      "         +- Scan parquet  (1)\n",
      "\n",
      "\n",
      "(1) Scan parquet \n",
      "Output [1]: [Call_Type#3]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [hdfs://nn:9000/sf.parquet]\n",
      "ReadSchema: struct<Call_Type:string>\n",
      "\n",
      "(2) HashAggregate\n",
      "Input [1]: [Call_Type#3]\n",
      "Keys [1]: [Call_Type#3]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#23405L]\n",
      "Results [2]: [Call_Type#3, count#23406L]\n",
      "\n",
      "(3) Exchange\n",
      "Input [2]: [Call_Type#3, count#23406L]\n",
      "Arguments: hashpartitioning(Call_Type#3, 200), ENSURE_REQUIREMENTS, [plan_id=839]\n",
      "\n",
      "(4) HashAggregate\n",
      "Input [2]: [Call_Type#3, count#23406L]\n",
      "Keys [1]: [Call_Type#3]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#23402L]\n",
      "Results [2]: [Call_Type#3, count(1)#23402L AS count#23401L]\n",
      "\n",
      "(5) AdaptiveSparkPlan\n",
      "Output [2]: [Call_Type#3, count#23401L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can see the physical plan for Spark queries using .explain()\n",
    "# Let's use this to see how Spark will handle the GROUP BY in this query\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    Call_Type\n",
    "    , COUNT(*) as count\n",
    "FROM\n",
    "    calls\n",
    "GROUP BY\n",
    "    Call_Type /* Force the shuffling for sake of demo */\n",
    "\"\"\").explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879e5c88-f38a-49e8-bb59-778eefc82e95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
