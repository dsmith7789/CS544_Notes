{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf657b52-62c1-466e-bc9b-c441d9ff426a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 521M\n",
      "drwxr-xr-x 3 root root 4.0K Nov  2 19:34 .\n",
      "drwxr-xr-x 1 root root 4.0K Nov  2 02:37 ..\n",
      "drwxr-xr-x 2 root root 4.0K Nov  2 19:32 .ipynb_checkpoints\n",
      "-rw-r--r-- 1 root root 2.3K Nov  2 19:34 Untitled.ipynb\n",
      "-rw-r--r-- 1 root root  11M Jan 12  2023 ghcnd-stations.txt\n",
      "-rw-r--r-- 1 root root 511M Mar  4  2023 sf.zip\n",
      "-rw-r--r-- 1 root root  21K Nov  2 19:31 spark_dataframes_1.ipynb\n",
      "-rw-r--r-- 1 root root  36K Nov  2 02:50 spark_demo_1.ipynb\n",
      "-rw-r--r-- 1 root root  13K Nov  2 03:48 spark_demo_2.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls -lah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d09ae4-6b1d-411f-a959-80ddaec94239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-02 19:33:37--  https://pages.cs.wisc.edu/~harter/cs544/data/sf.zip\n",
      "Resolving pages.cs.wisc.edu (pages.cs.wisc.edu)... 128.105.7.9\n",
      "Connecting to pages.cs.wisc.edu (pages.cs.wisc.edu)|128.105.7.9|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 534803160 (510M) [application/zip]\n",
      "Saving to: ‘sf.zip’\n",
      "\n",
      "sf.zip              100%[===================>] 510.03M  8.14MB/s    in 38s     \n",
      "\n",
      "2023-11-02 19:34:15 (13.5 MB/s) - ‘sf.zip’ saved [534803160/534803160]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -nc https://pages.cs.wisc.edu/~harter/cs544/data/sf.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c110dbd-5842-4645-a756-19bba5aa5da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/02 19:38:44 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# The Spark session is a little different this time\n",
    "# adding the Hive warehouse option (treat HDFS files as if they are SQL tables in a database)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .master(\"spark://boss:7077\")\n",
    "         .config(\"spark.executor.memory\", \"512MB\")   # needed to increase this memory to fix the error (minimum of 450 MB)\n",
    "         .config(\"spark.sql.warehouse.dir\", \"hdfs://nn:9000/user/hive/warehouse\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6757fe2-578f-455d-8b25-45f0559259bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# minimum Spark executor memory needed (from error) in MB = 450 MB\n",
    "471859200 / 1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f65290b-127c-4d27-8e50-eae63c49eb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "Suggested packages:\n",
      "  zip\n",
      "The following NEW packages will be installed:\n",
      "  unzip\n",
      "0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\n",
      "Need to get 174 kB of archives.\n",
      "After this operation, 385 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 unzip amd64 6.0-26ubuntu3.1 [174 kB]\n",
      "Fetched 174 kB in 1s (242 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package unzip.\n",
      "(Reading database ... 20105 files and directories currently installed.)\n",
      "Preparing to unpack .../unzip_6.0-26ubuntu3.1_amd64.deb ...\n",
      "Unpacking unzip (6.0-26ubuntu3.1) ...\n",
      "Setting up unzip (6.0-26ubuntu3.1) ...\n"
     ]
    }
   ],
   "source": [
    "! apt-get install unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1ea9682-7a8b-4888-8e07-be5d5dfff49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  sf.zip\n",
      "  inflating: sf.csv                  \n"
     ]
    }
   ],
   "source": [
    "! unzip sf.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a58d3d30-4618-440f-87d9-2e852eced5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cp sf.csv hdfs://nn:9000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bc9b9d2-e2df-4386-8116-b582d40482f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 root supergroup   10607756 2023-11-02 04:06 hdfs://nn:9000/ghcnd-stations.txt\n",
      "-rw-r--r--   3 root supergroup 2265533682 2023-11-02 20:24 hdfs://nn:9000/sf.csv\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls hdfs://nn:9000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed3608a5-a348-46fe-9eaf-d58a5c0205a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").load(\"hdfs://nn:9000/sf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a1b446e-dd65-4a34-a7be-8970bbd18346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/02 20:40:34 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>_c1</th>\n",
       "      <th>_c2</th>\n",
       "      <th>_c3</th>\n",
       "      <th>_c4</th>\n",
       "      <th>_c5</th>\n",
       "      <th>_c6</th>\n",
       "      <th>_c7</th>\n",
       "      <th>_c8</th>\n",
       "      <th>_c9</th>\n",
       "      <th>...</th>\n",
       "      <th>_c25</th>\n",
       "      <th>_c26</th>\n",
       "      <th>_c27</th>\n",
       "      <th>_c28</th>\n",
       "      <th>_c29</th>\n",
       "      <th>_c30</th>\n",
       "      <th>_c31</th>\n",
       "      <th>_c32</th>\n",
       "      <th>_c33</th>\n",
       "      <th>_c34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Call Number</td>\n",
       "      <td>Unit ID</td>\n",
       "      <td>Incident Number</td>\n",
       "      <td>Call Type</td>\n",
       "      <td>Call Date</td>\n",
       "      <td>Watch Date</td>\n",
       "      <td>Received DtTm</td>\n",
       "      <td>Entry DtTm</td>\n",
       "      <td>Dispatch DtTm</td>\n",
       "      <td>Response DtTm</td>\n",
       "      <td>...</td>\n",
       "      <td>Call Type Group</td>\n",
       "      <td>Number of Alarms</td>\n",
       "      <td>Unit Type</td>\n",
       "      <td>Unit sequence in call dispatch</td>\n",
       "      <td>Fire Prevention District</td>\n",
       "      <td>Supervisor District</td>\n",
       "      <td>Neighborhooods - Analysis Boundaries</td>\n",
       "      <td>RowID</td>\n",
       "      <td>case_location</td>\n",
       "      <td>Analysis Neighborhoods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221210313</td>\n",
       "      <td>E36</td>\n",
       "      <td>22054955</td>\n",
       "      <td>Outside Fire</td>\n",
       "      <td>05/01/2022</td>\n",
       "      <td>04/30/2022</td>\n",
       "      <td>05/01/2022 02:58:25 AM</td>\n",
       "      <td>05/01/2022 02:59:15 AM</td>\n",
       "      <td>05/01/2022 02:59:25 AM</td>\n",
       "      <td>05/01/2022 03:01:06 AM</td>\n",
       "      <td>...</td>\n",
       "      <td>Fire</td>\n",
       "      <td>1</td>\n",
       "      <td>ENGINE</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Hayes Valley</td>\n",
       "      <td>221210313-E36</td>\n",
       "      <td>POINT (-122.42316555403964 37.77781524520032)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>220190150</td>\n",
       "      <td>E29</td>\n",
       "      <td>22008871</td>\n",
       "      <td>Alarms</td>\n",
       "      <td>01/19/2022</td>\n",
       "      <td>01/18/2022</td>\n",
       "      <td>01/19/2022 01:42:12 AM</td>\n",
       "      <td>01/19/2022 01:44:13 AM</td>\n",
       "      <td>01/19/2022 01:44:28 AM</td>\n",
       "      <td>01/19/2022 01:46:47 AM</td>\n",
       "      <td>...</td>\n",
       "      <td>Alarm</td>\n",
       "      <td>1</td>\n",
       "      <td>ENGINE</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>Potrero Hill</td>\n",
       "      <td>220190150-E29</td>\n",
       "      <td>POINT (-122.39469970274361 37.76460987856451)</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           _c0      _c1              _c2           _c3         _c4  \\\n",
       "0  Call Number  Unit ID  Incident Number     Call Type   Call Date   \n",
       "1    221210313      E36         22054955  Outside Fire  05/01/2022   \n",
       "2    220190150      E29         22008871        Alarms  01/19/2022   \n",
       "\n",
       "          _c5                     _c6                     _c7  \\\n",
       "0  Watch Date           Received DtTm              Entry DtTm   \n",
       "1  04/30/2022  05/01/2022 02:58:25 AM  05/01/2022 02:59:15 AM   \n",
       "2  01/18/2022  01/19/2022 01:42:12 AM  01/19/2022 01:44:13 AM   \n",
       "\n",
       "                      _c8                     _c9  ...             _c25  \\\n",
       "0           Dispatch DtTm           Response DtTm  ...  Call Type Group   \n",
       "1  05/01/2022 02:59:25 AM  05/01/2022 03:01:06 AM  ...             Fire   \n",
       "2  01/19/2022 01:44:28 AM  01/19/2022 01:46:47 AM  ...            Alarm   \n",
       "\n",
       "               _c26       _c27                            _c28  \\\n",
       "0  Number of Alarms  Unit Type  Unit sequence in call dispatch   \n",
       "1                 1     ENGINE                               1   \n",
       "2                 1     ENGINE                               1   \n",
       "\n",
       "                       _c29                 _c30  \\\n",
       "0  Fire Prevention District  Supervisor District   \n",
       "1                         2                    5   \n",
       "2                         3                   10   \n",
       "\n",
       "                                   _c31           _c32  \\\n",
       "0  Neighborhooods - Analysis Boundaries          RowID   \n",
       "1                          Hayes Valley  221210313-E36   \n",
       "2                          Potrero Hill  220190150-E29   \n",
       "\n",
       "                                            _c33                    _c34  \n",
       "0                                  case_location  Analysis Neighborhoods  \n",
       "1  POINT (-122.42316555403964 37.77781524520032)                       9  \n",
       "2  POINT (-122.39469970274361 37.76460987856451)                      26  \n",
       "\n",
       "[3 rows x 35 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(3).toPandas()    # Header row not being treated as a header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7411125-6174-4b6b-8739-340f3d0dca9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#df = spark.read.format(\"csv\").option(KEY, VALUE).load(\"hdfs://nn:9000/sf.csv\")\n",
    "df = (spark.read.format(\"csv\")\n",
    "      .option(\"header\", True)\n",
    "      .load(\"hdfs://nn:9000/sf.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a686839-6265-43a6-84b2-d9d5432c5aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Call Number</th>\n",
       "      <th>Unit ID</th>\n",
       "      <th>Incident Number</th>\n",
       "      <th>Call Type</th>\n",
       "      <th>Call Date</th>\n",
       "      <th>Watch Date</th>\n",
       "      <th>Received DtTm</th>\n",
       "      <th>Entry DtTm</th>\n",
       "      <th>Dispatch DtTm</th>\n",
       "      <th>Response DtTm</th>\n",
       "      <th>...</th>\n",
       "      <th>Call Type Group</th>\n",
       "      <th>Number of Alarms</th>\n",
       "      <th>Unit Type</th>\n",
       "      <th>Unit sequence in call dispatch</th>\n",
       "      <th>Fire Prevention District</th>\n",
       "      <th>Supervisor District</th>\n",
       "      <th>Neighborhooods - Analysis Boundaries</th>\n",
       "      <th>RowID</th>\n",
       "      <th>case_location</th>\n",
       "      <th>Analysis Neighborhoods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221210313</td>\n",
       "      <td>E36</td>\n",
       "      <td>22054955</td>\n",
       "      <td>Outside Fire</td>\n",
       "      <td>05/01/2022</td>\n",
       "      <td>04/30/2022</td>\n",
       "      <td>05/01/2022 02:58:25 AM</td>\n",
       "      <td>05/01/2022 02:59:15 AM</td>\n",
       "      <td>05/01/2022 02:59:25 AM</td>\n",
       "      <td>05/01/2022 03:01:06 AM</td>\n",
       "      <td>...</td>\n",
       "      <td>Fire</td>\n",
       "      <td>1</td>\n",
       "      <td>ENGINE</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Hayes Valley</td>\n",
       "      <td>221210313-E36</td>\n",
       "      <td>POINT (-122.42316555403964 37.77781524520032)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>220190150</td>\n",
       "      <td>E29</td>\n",
       "      <td>22008871</td>\n",
       "      <td>Alarms</td>\n",
       "      <td>01/19/2022</td>\n",
       "      <td>01/18/2022</td>\n",
       "      <td>01/19/2022 01:42:12 AM</td>\n",
       "      <td>01/19/2022 01:44:13 AM</td>\n",
       "      <td>01/19/2022 01:44:28 AM</td>\n",
       "      <td>01/19/2022 01:46:47 AM</td>\n",
       "      <td>...</td>\n",
       "      <td>Alarm</td>\n",
       "      <td>1</td>\n",
       "      <td>ENGINE</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>Potrero Hill</td>\n",
       "      <td>220190150-E29</td>\n",
       "      <td>POINT (-122.39469970274361 37.76460987856451)</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>211233271</td>\n",
       "      <td>T07</td>\n",
       "      <td>21053032</td>\n",
       "      <td>Alarms</td>\n",
       "      <td>05/03/2021</td>\n",
       "      <td>05/03/2021</td>\n",
       "      <td>05/03/2021 09:28:12 PM</td>\n",
       "      <td>05/03/2021 09:28:12 PM</td>\n",
       "      <td>05/03/2021 09:28:17 PM</td>\n",
       "      <td>05/03/2021 09:29:10 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>Alarm</td>\n",
       "      <td>1</td>\n",
       "      <td>TRUCK</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>Mission</td>\n",
       "      <td>211233271-T07</td>\n",
       "      <td>POINT (-122.42057572093252 37.76418194637148)</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Call Number Unit ID Incident Number     Call Type   Call Date  Watch Date  \\\n",
       "0   221210313     E36        22054955  Outside Fire  05/01/2022  04/30/2022   \n",
       "1   220190150     E29        22008871        Alarms  01/19/2022  01/18/2022   \n",
       "2   211233271     T07        21053032        Alarms  05/03/2021  05/03/2021   \n",
       "\n",
       "            Received DtTm              Entry DtTm           Dispatch DtTm  \\\n",
       "0  05/01/2022 02:58:25 AM  05/01/2022 02:59:15 AM  05/01/2022 02:59:25 AM   \n",
       "1  01/19/2022 01:42:12 AM  01/19/2022 01:44:13 AM  01/19/2022 01:44:28 AM   \n",
       "2  05/03/2021 09:28:12 PM  05/03/2021 09:28:12 PM  05/03/2021 09:28:17 PM   \n",
       "\n",
       "            Response DtTm  ... Call Type Group Number of Alarms Unit Type  \\\n",
       "0  05/01/2022 03:01:06 AM  ...            Fire                1    ENGINE   \n",
       "1  01/19/2022 01:46:47 AM  ...           Alarm                1    ENGINE   \n",
       "2  05/03/2021 09:29:10 PM  ...           Alarm                1     TRUCK   \n",
       "\n",
       "  Unit sequence in call dispatch Fire Prevention District Supervisor District  \\\n",
       "0                              1                        2                   5   \n",
       "1                              1                        3                  10   \n",
       "2                              2                        2                   9   \n",
       "\n",
       "  Neighborhooods - Analysis Boundaries          RowID  \\\n",
       "0                         Hayes Valley  221210313-E36   \n",
       "1                         Potrero Hill  220190150-E29   \n",
       "2                              Mission  211233271-T07   \n",
       "\n",
       "                                   case_location Analysis Neighborhoods  \n",
       "0  POINT (-122.42316555403964 37.77781524520032)                      9  \n",
       "1  POINT (-122.39469970274361 37.76460987856451)                     26  \n",
       "2  POINT (-122.42057572093252 37.76418194637148)                     20  \n",
       "\n",
       "[3 rows x 35 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(3).toPandas()    #  Now the header is loaded in properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3eebe8f-367c-463a-8e3a-6454d16405f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Call Number: string, Unit ID: string, Incident Number: string, Call Type: string, Call Date: string, Watch Date: string, Received DtTm: string, Entry DtTm: string, Dispatch DtTm: string, Response DtTm: string, On Scene DtTm: string, Transport DtTm: string, Hospital DtTm: string, Call Final Disposition: string, Available DtTm: string, Address: string, City: string, Zipcode of Incident: string, Battalion: string, Station Area: string, Box: string, Original Priority: string, Priority: string, Final Priority: string, ALS Unit: string, Call Type Group: string, Number of Alarms: string, Unit Type: string, Unit sequence in call dispatch: string, Fire Prevention District: string, Supervisor District: string, Neighborhooods - Analysis Boundaries: string, RowID: string, case_location: string, Analysis Neighborhoods: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df # but everything is a string (schema inference is not done by default because it's expensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e25f6bf8-cd6e-4a51-88d5-87ecea1d614f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# This operation is much slower because we need a separate Spark job to do the schema inference\n",
    "# 17 tasks total (17 partitions, Spark automatically chooses based on file size and workers available in the cluster)\n",
    "df = (spark.read.format(\"csv\")\n",
    "      .option(\"header\", True)\n",
    "      .option(\"inferSchema\", True)\n",
    "      .load(\"hdfs://nn:9000/sf.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "299b137a-e6e4-4719-8feb-ca8677435fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Call Number: int, Unit ID: string, Incident Number: int, Call Type: string, Call Date: string, Watch Date: string, Received DtTm: string, Entry DtTm: string, Dispatch DtTm: string, Response DtTm: string, On Scene DtTm: string, Transport DtTm: string, Hospital DtTm: string, Call Final Disposition: string, Available DtTm: string, Address: string, City: string, Zipcode of Incident: int, Battalion: string, Station Area: string, Box: string, Original Priority: string, Priority: string, Final Priority: int, ALS Unit: boolean, Call Type Group: string, Number of Alarms: int, Unit Type: string, Unit sequence in call dispatch: int, Fire Prevention District: string, Supervisor District: string, Neighborhooods - Analysis Boundaries: string, RowID: string, case_location: string, Analysis Neighborhoods: int]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df    # Spark Schema Inference is still not smart enough to convert the dates from string to actual dates, so we need to manually fix this\n",
    "# This is \"Transformation\" work (the T in ETL)\n",
    "# Load (L) is into Parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e34f012-9a79-40a2-9e94-72fa3eb023b4",
   "metadata": {},
   "source": [
    "#### How to transform data with functions on columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "058536df-4377-438b-b214-1188b137f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6d4ee34-f4e5-4de0-afda-6d59225649dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Call Date'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col(\"Call Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca63a7d7-e2d7-4742-9909-81b3724f54be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Call AS Date'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr(\"Call Date\")    # notice how expr thinks \"Date\" is an alias for \"Call\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "421d0baa-ebc7-429a-b743-c69fbf30ecc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Call Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>05/01/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/19/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05/03/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/20/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04/30/2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Call Date\n",
       "0  05/01/2022\n",
       "1  01/19/2022\n",
       "2  05/03/2021\n",
       "3  10/20/2021\n",
       "4  04/30/2022"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting a column (specify column name using pyspark.sql.functions.col() ) works similar to SQL\n",
    "example = (df.select(col(\"Call Date\"))\n",
    "            .limit(5)\n",
    "            .toPandas())\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b6a3835-0164-4010-b7b2-ddc9ed542ad6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Call` cannot be resolved. Did you mean one of the following? [`Box`, `City`, `Call Date`, `Call Type`, `RowID`].; line 1 pos 0;\n'Project ['Call AS Date#426]\n+- Relation [Call Number#348,Unit ID#349,Incident Number#350,Call Type#351,Call Date#352,Watch Date#353,Received DtTm#354,Entry DtTm#355,Dispatch DtTm#356,Response DtTm#357,On Scene DtTm#358,Transport DtTm#359,Hospital DtTm#360,Call Final Disposition#361,Available DtTm#362,Address#363,City#364,Zipcode of Incident#365,Battalion#366,Station Area#367,Box#368,Original Priority#369,Priority#370,Final Priority#371,... 11 more fields] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Be careful when using pyspark.sql.functions.expr\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m example \u001b[38;5;241m=\u001b[39m (\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCall Date\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m             \u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      4\u001b[0m             \u001b[38;5;241m.\u001b[39mtoPandas())\n\u001b[1;32m      5\u001b[0m example\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:3223\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3180\u001b[0m \n\u001b[1;32m   3181\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3221\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3223\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Call` cannot be resolved. Did you mean one of the following? [`Box`, `City`, `Call Date`, `Call Type`, `RowID`].; line 1 pos 0;\n'Project ['Call AS Date#426]\n+- Relation [Call Number#348,Unit ID#349,Incident Number#350,Call Type#351,Call Date#352,Watch Date#353,Received DtTm#354,Entry DtTm#355,Dispatch DtTm#356,Response DtTm#357,On Scene DtTm#358,Transport DtTm#359,Hospital DtTm#360,Call Final Disposition#361,Available DtTm#362,Address#363,City#364,Zipcode of Incident#365,Battalion#366,Station Area#367,Box#368,Original Priority#369,Priority#370,Final Priority#371,... 11 more fields] csv\n"
     ]
    }
   ],
   "source": [
    "# Be careful when using pyspark.sql.functions.expr(); we get an error because of the aliasing the expression is trying to do\n",
    "example = (df.select(expr(\"Call Date\") )\n",
    "            .limit(5)\n",
    "            .toPandas())\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "310b829e-22c2-43e1-a38b-0adc60d6ed89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Call Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>05/01/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/19/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05/03/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/20/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04/30/2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Call Date\n",
       "0  05/01/2022\n",
       "1  01/19/2022\n",
       "2  05/03/2021\n",
       "3  10/20/2021\n",
       "4  04/30/2022"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put your column name in backticks (`) when using pyspark.sql.functions.expr() if the column name has a space (good practice in general)\n",
    "example = (df.select(expr(\"`Call Date`\") )\n",
    "            .limit(5)\n",
    "            .toPandas())\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5f55417-afb4-4a80-bd93-c29e7dd9e801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>My Alias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>05/01/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/19/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05/03/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/20/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04/30/2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     My Alias\n",
       "0  05/01/2022\n",
       "1  01/19/2022\n",
       "2  05/03/2021\n",
       "3  10/20/2021\n",
       "4  04/30/2022"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can alias columns in Spark DataFrames too\n",
    "example = (df.select(expr(\"`Call Date`\").alias(\"My Alias\")) \n",
    "            .limit(5)\n",
    "            .toPandas())\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dba1e719-b9eb-4840-8f33-d297142f2344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column formatted as Date now</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-10-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Column formatted as Date now\n",
       "0                   2022-05-01\n",
       "1                   2022-01-19\n",
       "2                   2021-05-03\n",
       "3                   2021-10-20\n",
       "4                   2022-04-30"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's convert the data type from string to date\n",
    "example = (df.select(expr(\"to_date(`Call Date`, 'MM/dd/yyyy')\").alias(\"Column formatted as Date now\")) \n",
    "            .limit(5)\n",
    "            .toPandas())\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ea613-8564-451b-a7c9-b85c7325439a",
   "metadata": {},
   "source": [
    "#### Goal: Create a parquet file with this data, with no spaces in the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5a63891-e4ad-4ef0-9c8e-9333b012aadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Call Number', 'Unit ID', 'Incident Number', 'Call Type', 'Call Date']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[ : 5]    # a list of strings (the column headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "971bbbf0-80df-4686-a5f8-7cfa0d20c94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column<'Call Number'>,\n",
       " Column<'Unit ID'>,\n",
       " Column<'Incident Number'>,\n",
       " Column<'Call Type'>,\n",
       " Column<'Call Date'>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[col(c) for c in df.columns][ : 5]    # get a Column object for each of the column names in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "001f1429-c35f-4912-8597-557165580f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column<'Call Number AS Call_Number'>,\n",
       " Column<'Unit ID AS Unit_ID'>,\n",
       " Column<'Incident Number AS Incident_Number'>,\n",
       " Column<'Call Type AS Call_Type'>,\n",
       " Column<'Call Date AS Call_Date'>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [col(c).alias(c.replace(' ', '_')) for c in df.columns]    # rename the columns by removing space and replacing with underscore\n",
    "columns[ : 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f242107b-134e-4ba6-aa42-4c87f170f9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Call_Number: int, Unit_ID: string, Incident_Number: int, Call_Type: string, Call_Date: string, Watch_Date: string, Received_DtTm: string, Entry_DtTm: string, Dispatch_DtTm: string, Response_DtTm: string, On_Scene_DtTm: string, Transport_DtTm: string, Hospital_DtTm: string, Call_Final_Disposition: string, Available_DtTm: string, Address: string, City: string, Zipcode_of_Incident: int, Battalion: string, Station_Area: string, Box: string, Original_Priority: string, Priority: string, Final_Priority: int, ALS_Unit: boolean, Call_Type_Group: string, Number_of_Alarms: int, Unit_Type: string, Unit_sequence_in_call_dispatch: int, Fire_Prevention_District: string, Supervisor_District: string, Neighborhooods_-_Analysis_Boundaries: string, RowID: string, case_location: string, Analysis_Neighborhoods: int]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(columns)   # now our select is using the aliases we made and the output has no spaces in the column header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1016c1a9-efc1-4977-ac57-76ab6a663f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "92f91697-cd2c-4ff3-af1b-265a0d0065eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# let's load our dataframe with underscore headers into a parquet file\n",
    "# read data from file into a dataframe using spark.read (gives us a pyspark.sql.readwriter.DataFromeReader)\n",
    "# write data from dataframe into file using df.write (gives us a pyspark.sql.readwriter.DataFrameWriter)\n",
    "\n",
    "(df.select(columns)\n",
    " .limit(10)    # small amount to test\n",
    " .write\n",
    " .format(\"parquet\")\n",
    " .save(\"hdfs://nn:9000/sf.parquet\"))\n",
    "\n",
    "# 17 tasks because Spark uses 17 partitions for the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a0a353f-7b9c-4280-87ba-2692d61808bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path hdfs://nn:9000/sf.parquet already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Since the write for a small amount worked, let's try for the whole dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m (\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m----> 5\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhdfs://nn:9000/sf.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# ERROR: we have to specify if we want to overwrite the existing files\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py:1463\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1463\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path hdfs://nn:9000/sf.parquet already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "# Since the write for a small amount worked, let's try for the whole dataset\n",
    "(df.select(columns)\n",
    " .write\n",
    " .format(\"parquet\")\n",
    " .save(\"hdfs://nn:9000/sf.parquet\"))\n",
    "\n",
    "# ERROR: we have to specify if we want to overwrite the existing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7f61f82f-7b14-4a3c-9d28-e09110a5e9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(df.select(columns)\n",
    " .write\n",
    " .format(\"parquet\")\n",
    " .mode(\"overwrite\")    # could also be append (keep adding), ignore (do nothing if the file already exists), etc.\n",
    " .save(\"hdfs://nn:9000/sf.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1a368a62-52d2-40cb-9952-d5615a925e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   3 root supergroup   10607756 2023-11-02 04:06 hdfs://nn:9000/ghcnd-stations.txt\n",
      "-rw-r--r--   3 root supergroup 2265533682 2023-11-02 20:24 hdfs://nn:9000/sf.csv\n",
      "drwxr-xr-x   - root supergroup          0 2023-11-02 21:24 hdfs://nn:9000/sf.parquet\n"
     ]
    }
   ],
   "source": [
    "# So what do the parquet files that Spark wrote look like in HDFS?\n",
    "! hdfs dfs -ls hdfs://nn:9000/    \n",
    "\n",
    "# It's a directory called \"sf.parquet\", not a single file called \"sf.parquet\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd53c363-30de-47ed-8852-393e43258de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 items\n",
      "-rw-r--r--   3 root supergroup          0 2023-11-02 21:24 hdfs://nn:9000/sf.parquet/_SUCCESS\n",
      "-rw-r--r--   3 root supergroup   27805433 2023-11-02 21:22 hdfs://nn:9000/sf.parquet/part-00000-6630bd23-68db-484b-84f9-41b7807a17cc-c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup   27788944 2023-11-02 21:22 hdfs://nn:9000/sf.parquet/part-00001-6630bd23-68db-484b-84f9-41b7807a17cc-c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup   40477903 2023-11-02 21:22 hdfs://nn:9000/sf.parquet/part-00002-6630bd23-68db-484b-84f9-41b7807a17cc-c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup   36016086 2023-11-02 21:22 hdfs://nn:9000/sf.parquet/part-00003-6630bd23-68db-484b-84f9-41b7807a17cc-c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup   36031567 2023-11-02 21:22 hdfs://nn:9000/sf.parquet/part-00004-6630bd23-68db-484b-84f9-41b7807a17cc-c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup   36080343 2023-11-02 21:23 hdfs://nn:9000/sf.parquet/part-00005-6630bd23-68db-484b-84f9-41b7807a17cc-c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup   35943551 2023-11-02 21:23 hdfs://nn:9000/sf.parquet/part-00006-6630bd23-68db-484b-84f9-41b7807a17cc-c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup   35910610 2023-11-02 21:23 hdfs://nn:9000/sf.parquet/part-00007-6630bd23-68db-484b-84f9-41b7807a17cc-c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup   36435318 2023-11-02 21:23 hdfs://nn:9000/sf.parquet/part-00008-6630bd23-68db-484b-84f9-41b7807a17cc-c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup   35366522 2023-11-02 21:23 hdfs://nn:9000/sf.parquet/part-00009-6630bd23-68db-484b-84f9-41b7807a17cc-c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup   34237927 2023-11-02 21:23 hdfs://nn:9000/sf.parquet/part-00010-6630bd23-68db-484b-84f9-41b7807a17cc-c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup   33947693 2023-11-02 21:23 hdfs://nn:9000/sf.parquet/part-00011-6630bd23-68db-484b-84f9-41b7807a17cc-c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup   33487699 2023-11-02 21:23 hdfs://nn:9000/sf.parquet/part-00012-6630bd23-68db-484b-84f9-41b7807a17cc-c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup   34899250 2023-11-02 21:24 hdfs://nn:9000/sf.parquet/part-00013-6630bd23-68db-484b-84f9-41b7807a17cc-c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup   35715225 2023-11-02 21:24 hdfs://nn:9000/sf.parquet/part-00014-6630bd23-68db-484b-84f9-41b7807a17cc-c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup   35768659 2023-11-02 21:24 hdfs://nn:9000/sf.parquet/part-00015-6630bd23-68db-484b-84f9-41b7807a17cc-c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup   29362733 2023-11-02 21:24 hdfs://nn:9000/sf.parquet/part-00016-6630bd23-68db-484b-84f9-41b7807a17cc-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# we have 17 different files, 1 per partition/Spark task\n",
    "! hdfs dfs -ls hdfs://nn:9000/sf.parquet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "93c7cbe5-ded9-4021-b484-299589507d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark is designed to work with Parquet directories like this (this arrangement is quite common), so no need to combine\n",
    "# loading in the Parquet files is much faster than CSV (column format, compression)\n",
    "df = spark.read.format(\"parquet\").load(\"hdfs://nn:9000/sf.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d96e99ea-f2f2-4408-8566-0485c78a3efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Call_Number: int, Unit_ID: string, Incident_Number: int, Call_Type: string, Call_Date: string, Watch_Date: string, Received_DtTm: string, Entry_DtTm: string, Dispatch_DtTm: string, Response_DtTm: string, On_Scene_DtTm: string, Transport_DtTm: string, Hospital_DtTm: string, Call_Final_Disposition: string, Available_DtTm: string, Address: string, City: string, Zipcode_of_Incident: int, Battalion: string, Station_Area: string, Box: string, Original_Priority: string, Priority: string, Final_Priority: int, ALS_Unit: boolean, Call_Type_Group: string, Number_of_Alarms: int, Unit_Type: string, Unit_sequence_in_call_dispatch: int, Fire_Prevention_District: string, Supervisor_District: string, Neighborhooods_-_Analysis_Boundaries: string, RowID: string, case_location: string, Analysis_Neighborhoods: int]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8741fb68-ddb9-4500-b802-eb3927f95d2c",
   "metadata": {},
   "source": [
    "#### Generally, load CSV, transform data, write to Parquet, and then use Parquet for analysis exclusively!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dd5d2a86-ed5c-45bd-ae50-9a9e563aacd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Less partitions are needed (before when we read in text, numPartitions was 17) because of compression, column format (text is way bloated)\n",
    "# Spark is smart enough to realize that we can use fewer partitions than files because the files are smaller\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "052b74e5-471e-4d7f-9039-77d1c42de0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs://nn:9000/sf.csv\n"
     ]
    }
   ],
   "source": [
    "# at this point, we can remove the sf.csv from HDFS because we don't need it (have it in Parquet format), free up space\n",
    "! hdfs dfs -rm hdfs://nn:9000/sf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8e80670a-0190-47e4-8a01-29df4313a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"hdfs://nn:9000/sf.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "42c81ac8-bbd0-4460-b351-750987ad21c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"calls\")    # better than createView(), handles case where need to run code again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5f5e68-49c2-4605-88f0-f8343f5c5548",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
