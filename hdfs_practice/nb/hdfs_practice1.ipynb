{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0375c42f-f0e7-4d56-be37-3e0abc40a46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 25821052928 (24.05 GB)\n",
      "Present Capacity: 8874823680 (8.27 GB)\n",
      "DFS Remaining: 8874799104 (8.27 GB)\n",
      "DFS Used: 24576 (24 KB)\n",
      "DFS Used%: 0.00%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (1):\n",
      "\n",
      "Name: 172.18.0.3:9866 (hdfs_practice-hdfs-1.hdfs_practice_default)\n",
      "Hostname: main\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 25821052928 (24.05 GB)\n",
      "DFS Used: 24576 (24 KB)\n",
      "Non DFS Used: 16929452032 (15.77 GB)\n",
      "DFS Remaining: 8874799104 (8.27 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 34.37%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Fri Oct 20 00:58:19 GMT 2023\n",
      "Last Block Report: Fri Oct 20 00:49:25 GMT 2023\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a report of the cluster state\n",
    "!hdfs dfsadmin -fs hdfs://main:9000 -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71bb6f3d-39f2-459c-9c59-0689db79b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir hdfs://main:9000/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "649383d4-195b-46f0-97e3-dd60f0f0617b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                 Apache License\n",
      "                           Version 2.0, January 2004\n",
      "                        http://www.apache.org/licenses/\n",
      "\n",
      "   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n",
      "\n",
      "   1. Definitions.\n",
      "\n",
      "      \"License\" shall mean the terms and conditions for use, reproduction,\n"
     ]
    }
   ],
   "source": [
    "# As an example, let's load the software license (from notebook container) into hdfs\n",
    "!head /hadoop-3.3.6/LICENSE.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4596a3c-f2a0-4ff7-81b0-489a43035a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - root supergroup          0 2023-10-20 01:25 hdfs://main:9000/data\n"
     ]
    }
   ],
   "source": [
    "# What files do we have in hdfs so far?\n",
    "!hdfs dfs -ls hdfs://main:9000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe81179d-fd14-464b-a401-fa6bdb98b9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from container to hdfs\n",
    "!hdfs dfs -cp /hadoop-3.3.6/LICENSE.txt hdfs://main:9000/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36abe9c5-2d16-413b-b43c-5ee8cd0bbd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   3 root supergroup      15217 2023-10-20 01:29 hdfs://main:9000/data/LICENSE.txt\n"
     ]
    }
   ],
   "source": [
    "# Now the file shows in hdfs\n",
    "!hdfs dfs -ls hdfs://main:9000/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8aaa4a9-7f2a-413e-8a5d-9f185b2e29a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                 Apache License\n",
      "                           Version 2.0, January 2004\n",
      "                        http://www.apache.org/licenses/\n",
      "\n",
      "   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n",
      "\n",
      "   1. Definitions.\n",
      "\n",
      "      \"License\" shall mean the terms and conditions for use, reproduction,\n",
      "      and distribution as defined by Sections 1 through 9 of this document.\n",
      "\n",
      "      \"Licensor\" shall mean the copyright owner or entity authorized by\n",
      "      the copyright owner that is granting the License.\n",
      "\n",
      "      \"Legal Entity\" shall mean the union of the acting entity and all\n",
      "      other entities that control, are controlled by, or are under common\n",
      "      control with that entity. For the purposes of this definition,\n",
      "      \"control\" means (i) the power, direct or indirect, to cause the\n",
      "      direction or management of such entity, whether by contract or\n",
      "      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n",
      "      outstanding shares, or (iii) beneficial ownership of such entity.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can look at the contents (cat, head, tail) of the file we just loaded into hdfs\n",
    "!hdfs dfs -head hdfs://main:9000/data/LICENSE.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be8272df-d365-4d89-ad1a-1e69942ab5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15217  45651  hdfs://main:9000/data/LICENSE.txt\n"
     ]
    }
   ],
   "source": [
    "# how much disk space is being used?\n",
    "!hdfs dfs -du hdfs://main:9000/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb067c86-61d2-4c68-ac84-9616b5ec7b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we actually get 2 numbers (45651=physical bytes, 15217=logical bytes)\n",
    "# also note, 45651 is the \"expected\" physical bytes, but we only have 1 data node, therefore only 1 copy of the file\n",
    "45651 / 15217"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3af351fc-6ee4-4640-949b-601cb781a6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://main:9870/fsck?ugi=root&path=%2Fdata%2FLICENSE.txt\n",
      "FSCK started by root (auth:SIMPLE) from /172.18.0.2 for path /data/LICENSE.txt at Fri Oct 20 01:38:06 GMT 2023\n",
      "\n",
      "\n",
      "/data/LICENSE.txt:  Under replicated BP-1309233855-172.18.0.3-1697762626319:blk_1073741825_1001. Target Replicas is 3 but found 1 live replica(s), 0 decommissioned replica(s), 0 decommissioning replica(s).\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t1\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t0\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t15217 B\n",
      " Total files:\t1\n",
      " Total blocks (validated):\t1 (avg. block size 15217 B)\n",
      " Minimally replicated blocks:\t1 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t1 (100.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t3\n",
      " Average block replication:\t1.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t2 (66.666664 %)\n",
      " Blocks queued for replication:\t0\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      " Blocks queued for replication:\t0\n",
      "FSCK ended at Fri Oct 20 01:38:06 GMT 2023 in 3 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/data/LICENSE.txt' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "# fsck = file system checker; helps us figure out if there's an issue with the file (ex. we currently only have 1 replica when we need 3)\n",
    "# note the \"Missing Replicas\" in the Replicated Blocks section that indicates this problem; also block is \"Under-replicated\"\n",
    "!hdfs fsck hdfs://main:9000/data/LICENSE.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "919447d2-b3d5-4bad-a72b-2c92e3e4ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's copy the file in again, but make sure that hdfs doesn't think it's underreplicated this time (i.e. only expects 1 replica)\n",
    "# hdfs dfs <OPTIONS_TO_JAVA> -cp <src> <dst> \n",
    "!hdfs dfs -D dfs.replication=1 -cp /hadoop-3.3.6/LICENSE.txt hdfs://main:9000/data/v2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d7e2fc8-7d2f-4757-a578-a79baa18a2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://main:9870/fsck?ugi=root&path=%2Fdata%2Fv2.txt\n",
      "FSCK started by root (auth:SIMPLE) from /172.18.0.2 for path /data/v2.txt at Fri Oct 20 01:41:49 GMT 2023\n",
      "\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t1\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t0\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t15217 B\n",
      " Total files:\t1\n",
      " Total blocks (validated):\t1 (avg. block size 15217 B)\n",
      " Minimally replicated blocks:\t1 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t3\n",
      " Average block replication:\t1.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      " Blocks queued for replication:\t0\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      " Blocks queued for replication:\t0\n",
      "FSCK ended at Fri Oct 20 01:41:49 GMT 2023 in 3 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/data/v2.txt' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "# use fsck to verify that hdfs thinks this v2 copy is good to go\n",
    "!hdfs fsck hdfs://main:9000/data/v2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b884cb-dfae-49ec-b1a4-7ed99669c192",
   "metadata": {},
   "source": [
    "# WebHDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8943487b-2b51-449c-8bba-81e8a049032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://hadoop.apache.org/docs/r3.3.6/hadoop-project-dist/hadoop-hdfs/WebHDFS.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c57e37eb-d8eb-4b65-a1f0-8d3e3e969eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "\u001b[1mDate\u001b[0m: Fri, 20 Oct 2023 06:52:11 GMT\n",
      "\u001b[1mCache-Control\u001b[0m: no-cache\n",
      "\u001b[1mExpires\u001b[0m: Fri, 20 Oct 2023 06:52:14 GMT\n",
      "\u001b[1mDate\u001b[0m: Fri, 20 Oct 2023 06:52:14 GMT\n",
      "\u001b[1mPragma\u001b[0m: no-cache\n",
      "\u001b[1mX-Content-Type-Options\u001b[0m: nosniff\n",
      "\u001b[1mX-FRAME-OPTIONS\u001b[0m: SAMEORIGIN\n",
      "\u001b[1mX-XSS-Protection\u001b[0m: 1; mode=block\n",
      "\u001b[1mContent-Type\u001b[0m: application/json\n",
      "\u001b[1mTransfer-Encoding\u001b[0m: chunked\n",
      "\n",
      "{\"FileStatuses\":{\"FileStatus\":[\n",
      "{\"accessTime\":1697765385136,\"blockSize\":134217728,\"childrenNum\":0,\"fileId\":16387,\"group\":\"supergroup\",\"length\":15217,\"modificationTime\":1697765386031,\"owner\":\"root\",\"pathSuffix\":\"LICENSE.txt\",\"permission\":\"644\",\"replication\":3,\"storagePolicy\":0,\"type\":\"FILE\"},\n",
      "{\"accessTime\":1697766068128,\"blockSize\":134217728,\"childrenNum\":0,\"fileId\":16388,\"group\":\"supergroup\",\"length\":15217,\"modificationTime\":1697766068361,\"owner\":\"root\",\"pathSuffix\":\"v2.txt\",\"permission\":\"644\",\"replication\":1,\"storagePolicy\":0,\"type\":\"FILE\"}\n",
      "]}}\n"
     ]
    }
   ],
   "source": [
    "# Example 1: List a directory: https://hadoop.apache.org/docs/r3.3.6/hadoop-project-dist/hadoop-hdfs/WebHDFS.html#List_a_Directory\n",
    "# NOTE: we changed ports (using 9870 for WebHDFS/REST APIs, not 9000 like we do for regular command line)\n",
    "# It looks like \"hdfs://\" is replaced with \"webhdfs/v1\"\n",
    "\n",
    "! curl -i  \"http://main:9870/webhdfs/v1/data?op=LISTSTATUS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30f37bbe-570d-4e9d-927e-a5db72067511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"FileStatuses\":{\"FileStatus\":[\n",
      "{\"accessTime\":1697765385136,\"blockSize\":134217728,\"childrenNum\":0,\"fileId\":16387,\"group\":\"supergroup\",\"length\":15217,\"modificationTime\":1697765386031,\"owner\":\"root\",\"pathSuffix\":\"LICENSE.txt\",\"permission\":\"644\",\"replication\":3,\"storagePolicy\":0,\"type\":\"FILE\"},\n",
      "{\"accessTime\":1697766068128,\"blockSize\":134217728,\"childrenNum\":0,\"fileId\":16388,\"group\":\"supergroup\",\"length\":15217,\"modificationTime\":1697766068361,\"owner\":\"root\",\"pathSuffix\":\"v2.txt\",\"permission\":\"644\",\"replication\":1,\"storagePolicy\":0,\"type\":\"FILE\"}\n",
      "]}}\n"
     ]
    }
   ],
   "source": [
    "# -i flag in curl gives the headers, if we remove it we get the plain JSON\n",
    "! curl \"http://main:9870/webhdfs/v1/data?op=LISTSTATUS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f6e024c-45d6-4a02-bf18-bc525cae45bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 307 Temporary Redirect\n",
      "\u001b[1mDate\u001b[0m: Fri, 20 Oct 2023 06:57:54 GMT\n",
      "\u001b[1mCache-Control\u001b[0m: no-cache\n",
      "\u001b[1mExpires\u001b[0m: Fri, 20 Oct 2023 06:57:54 GMT\n",
      "\u001b[1mDate\u001b[0m: Fri, 20 Oct 2023 06:57:54 GMT\n",
      "\u001b[1mPragma\u001b[0m: no-cache\n",
      "\u001b[1mX-Content-Type-Options\u001b[0m: nosniff\n",
      "\u001b[1mX-FRAME-OPTIONS\u001b[0m: SAMEORIGIN\n",
      "\u001b[1mX-XSS-Protection\u001b[0m: 1; mode=block\n",
      "\u001b[1mLocation\u001b[0m: http://main:9864/webhdfs/v1/data/v2.txt?op=OPEN&namenoderpcaddress=main:9000&length=200&offset=0\n",
      "\u001b[1mContent-Type\u001b[0m: application/octet-stream\n",
      "\u001b[1mContent-Length\u001b[0m: 0\n",
      "\n",
      "HTTP/1.1 200 OK\n",
      "\u001b[1mAccess-Control-Allow-Methods\u001b[0m: GET\n",
      "\u001b[1mAccess-Control-Allow-Origin\u001b[0m: *\n",
      "\u001b[1mContent-Type\u001b[0m: application/octet-stream\n",
      "\u001b[1mConnection\u001b[0m: close\n",
      "\u001b[1mContent-Length\u001b[0m: 200\n",
      "\n",
      "\n",
      "                                 Apache License\n",
      "                           Version 2.0, January 2004\n",
      "                        http://www.apache.org/licenses/\n",
      "\n",
      "   TERMS AND CONDITIONS FOR USE, REPRODUC"
     ]
    }
   ],
   "source": [
    "# Example 2: Access a file: https://hadoop.apache.org/docs/r3.3.6/hadoop-project-dist/hadoop-hdfs/WebHDFS.html#Open_and_Read_a_File\n",
    "# [] means optional\n",
    "# Read the first 100 bytes of the file\n",
    "! curl -i -L \"http://main:9870/webhdfs/v1/data/v2.txt?op=OPEN&offset=0&length=200\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5aa0b8b8-b4dc-40d9-9a82-05d65e92374d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 307 Temporary Redirect\n",
      "\u001b[1mDate\u001b[0m: Fri, 20 Oct 2023 06:58:06 GMT\n",
      "\u001b[1mCache-Control\u001b[0m: no-cache\n",
      "\u001b[1mExpires\u001b[0m: Fri, 20 Oct 2023 06:58:06 GMT\n",
      "\u001b[1mDate\u001b[0m: Fri, 20 Oct 2023 06:58:06 GMT\n",
      "\u001b[1mPragma\u001b[0m: no-cache\n",
      "\u001b[1mX-Content-Type-Options\u001b[0m: nosniff\n",
      "\u001b[1mX-FRAME-OPTIONS\u001b[0m: SAMEORIGIN\n",
      "\u001b[1mX-XSS-Protection\u001b[0m: 1; mode=block\n",
      "\u001b[1mLocation\u001b[0m: http://main:9864/webhdfs/v1/data/v2.txt?op=OPEN&namenoderpcaddress=main:9000&length=200&offset=1\n",
      "\u001b[1mContent-Type\u001b[0m: application/octet-stream\n",
      "\u001b[1mContent-Length\u001b[0m: 0\n",
      "\n",
      "HTTP/1.1 200 OK\n",
      "\u001b[1mAccess-Control-Allow-Methods\u001b[0m: GET\n",
      "\u001b[1mAccess-Control-Allow-Origin\u001b[0m: *\n",
      "\u001b[1mContent-Type\u001b[0m: application/octet-stream\n",
      "\u001b[1mConnection\u001b[0m: close\n",
      "\u001b[1mContent-Length\u001b[0m: 200\n",
      "\n",
      "                                 Apache License\n",
      "                           Version 2.0, January 2004\n",
      "                        http://www.apache.org/licenses/\n",
      "\n",
      "   TERMS AND CONDITIONS FOR USE, REPRODUCT"
     ]
    }
   ],
   "source": [
    "# Notice how if we keep the length and change the offset, the printout changes (by 1 letter)\n",
    "! curl -i -L \"http://main:9870/webhdfs/v1/data/v2.txt?op=OPEN&offset=1&length=200\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9fea3e25-611c-475f-b587-01747e1b6f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 307 Temporary Redirect\n",
      "\u001b[1mDate\u001b[0m: Fri, 20 Oct 2023 07:00:55 GMT\n",
      "\u001b[1mCache-Control\u001b[0m: no-cache\n",
      "\u001b[1mExpires\u001b[0m: Fri, 20 Oct 2023 07:00:55 GMT\n",
      "\u001b[1mDate\u001b[0m: Fri, 20 Oct 2023 07:00:55 GMT\n",
      "\u001b[1mPragma\u001b[0m: no-cache\n",
      "\u001b[1mX-Content-Type-Options\u001b[0m: nosniff\n",
      "\u001b[1mX-FRAME-OPTIONS\u001b[0m: SAMEORIGIN\n",
      "\u001b[1mX-XSS-Protection\u001b[0m: 1; mode=block\n",
      "\u001b[1mLocation\u001b[0m: http://main:9864/webhdfs/v1/data/v2.txt?op=OPEN&namenoderpcaddress=main:9000&length=200&offset=1\n",
      "\u001b[1mContent-Type\u001b[0m: application/octet-stream\n",
      "\u001b[1mContent-Length\u001b[0m: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# curl -L means \"follow redirect\". When we send a request to the name node, it uses HTTP redirect to direct our \n",
    "# request to an appropriate data node\n",
    "# For example, if we don't use -L, we don't get any data back (the headers are because of the -i flag)\n",
    "! curl -i \"http://main:9870/webhdfs/v1/data/v2.txt?op=OPEN&offset=1&length=200\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fec05c50-004b-4838-bdd4-3fbd18ff25d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "\u001b[1mDate\u001b[0m: Fri, 20 Oct 2023 07:03:25 GMT\n",
      "\u001b[1mCache-Control\u001b[0m: no-cache\n",
      "\u001b[1mExpires\u001b[0m: Fri, 20 Oct 2023 07:03:25 GMT\n",
      "\u001b[1mDate\u001b[0m: Fri, 20 Oct 2023 07:03:25 GMT\n",
      "\u001b[1mPragma\u001b[0m: no-cache\n",
      "\u001b[1mX-Content-Type-Options\u001b[0m: nosniff\n",
      "\u001b[1mX-FRAME-OPTIONS\u001b[0m: SAMEORIGIN\n",
      "\u001b[1mX-XSS-Protection\u001b[0m: 1; mode=block\n",
      "\u001b[1mContent-Type\u001b[0m: application/json\n",
      "\u001b[1mTransfer-Encoding\u001b[0m: chunked\n",
      "\n",
      "{\"Location\":\"http://main:9864/webhdfs/v1/data/v2.txt?op=OPEN&namenoderpcaddress=main:9000&length=200&offset=1\"}"
     ]
    }
   ],
   "source": [
    "# Using the noredirect in the URL just gives us the location of the blocks we're trying to access (using the offset and length)\n",
    "! curl -i \"http://main:9870/webhdfs/v1/data/v2.txt?op=OPEN&offset=1&length=200&noredirect=true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031381c3-95b1-4e53-88dc-be5732b63cdf",
   "metadata": {},
   "source": [
    "# Using Python requests module instead of curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26ce191c-143e-4a90-80df-6287508a372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0393a2d4-28fc-4459-906f-9666b6c64825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://main:9864/webhdfs/v1/data/v2.txt?op=OPEN&namenoderpcaddress=main:9000&length=200&offset=1'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(\"http://main:9870/webhdfs/v1/data/v2.txt?op=OPEN&offset=1&length=200&noredirect=true\")\n",
    "r.raise_for_status()  # raise an exception if we didn't get a return code of 200 (success)\n",
    "# r.json()  # convert the request response from bytes to JSON\n",
    "r.json()[\"Location\"]  # now that the response is in JSON, use standard JSON syntax to access an value by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "30dffc0a-85ba-4566-8717-345af038c94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCT'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(\"http://main:9870/webhdfs/v1/data/v2.txt?op=OPEN&offset=1&length=200\")\n",
    "r.raise_for_status()  \n",
    "r.content  # this response doesn't give JSON (we omitted the noredirect=true portion of the query) so using the .json() method won't work here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340775ed-afb0-4e5c-aea8-eb2dc56b2741",
   "metadata": {},
   "source": [
    "# PyArrow - for easier working with files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31eb4d0e-5775-4482-b630-90ad1327d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bfa6503b-9a5d-4eee-b43e-32f4abaa18d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-20 07:15:35,134 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# pa.fs.HadoopFileSystem(<HOST>, <PORT>)\n",
    "# also note we're not using WebHDFS here, so just use port 9000 again (not 9870)\n",
    "hdfs = pa.fs.HadoopFileSystem(\"main\", 9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "feb3071a-9fef-4624-bb6b-258265a488e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = hdfs.open_input_file(\"/data/v2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5c73bb0e-7fcd-4b99-a827-be45ade6b6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.lib.NativeFile"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "86c77001-6797-4902-b1a4-ba99f644d14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyarrow.lib.NativeFile, pyarrow.lib._Weakrefable, object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(f).__mro__  # method resolution order, can tell us about method inheritance, if applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a92090f-9499-4542-9d37-76c2b909c648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__next__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '_assert_open',\n",
       " '_assert_readable',\n",
       " '_assert_seekable',\n",
       " '_assert_writable',\n",
       " '_default_chunk_size',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'download',\n",
       " 'fileno',\n",
       " 'flush',\n",
       " 'get_stream',\n",
       " 'isatty',\n",
       " 'metadata',\n",
       " 'mode',\n",
       " 'read',\n",
       " 'read1',\n",
       " 'read_at',\n",
       " 'read_buffer',\n",
       " 'readable',\n",
       " 'readall',\n",
       " 'readinto',\n",
       " 'readline',\n",
       " 'readlines',\n",
       " 'seek',\n",
       " 'seekable',\n",
       " 'size',\n",
       " 'tell',\n",
       " 'truncate',\n",
       " 'upload',\n",
       " 'writable',\n",
       " 'write',\n",
       " 'writelines']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(f)  # tells us the methods and attributes available with this f object\n",
    "# reviewing the methods and attributes tells us that this is similar to a Python raw file (no text, just bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6ec9a98e-f3f1-409e-af9c-dcfe16373982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that i'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.read_at(500, 0)  # we can see the 500 bytes starting at offset 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3c995a44-8d53-4f78-bca2-418a23184ce7",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# but we can't loop over it in this format --> notice how this gives an error\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(line)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/io.pxi:572\u001b[0m, in \u001b[0;36mpyarrow.lib.NativeFile.__next__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/io.pxi:555\u001b[0m, in \u001b[0;36mpyarrow.lib.NativeFile.readline\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnsupportedOperation\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# but we can't loop over it in this format --> notice how this gives an error\n",
    "for line in f:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aaf3d114-31a2-41f2-96b9-92d823891a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0ae77ea4-9568-44ce-8409-663d4fb2628d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\n'\n"
     ]
    }
   ],
   "source": [
    "reader = io.BufferedReader(f)\n",
    "for line in reader:\n",
    "    print(line)  # line is still in bytes if we do this\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "390e319c-6351-4f62-963d-19198834647c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                                 Apache License\n",
      "\n",
      "                           Version 2.0, January 2004\n",
      "\n",
      "                        http://www.apache.org/licenses/\n",
      "\n",
      "\n",
      "\n",
      "   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n",
      "\n",
      "\n",
      "\n",
      "   1. Definitions.\n",
      "\n",
      "\n",
      "\n",
      "      \"License\" shall mean the terms and conditions for use, reproduction,\n",
      "\n",
      "      and distribution as defined by Sections 1 through 9 of this document.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now we can loop over the file as if it's text\n",
    "with hdfs.open_input_file(\"/data/v2.txt\") as f:\n",
    "    reader = io.TextIOWrapper(io.BufferedReader(f))\n",
    "    for i, line in enumerate(reader):\n",
    "        print(line)\n",
    "        if i > 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9d70b5-076e-46bb-bf13-0816abfb7944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
